[core]
# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor, KubernetesExecutor
executor = CeleryExecutor

# Do not load examples that ship with Airflow.
load_examples = False

# Secret key to save connection passwords in the db
fernet_key = V9hpj_BPA6Sc9NSxh5NI2doKxfIQJOCHIGZjJqA-ZpA=

# Whether to disable pickling dags
donot_pickle = True

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their website
sql_alchemy_conn = mysql+mysqldb://airflow:password@airflow-db.airflow.svc.cluster.local/airflow

# The amount of parallelism as a setting to the executor. This defines
# the max number of task instances that should run simultaneously
# on this airflow installation
parallelism = 32

[webserver]
# Secret key used to run your flask app
secret_key = myverysecretkey
rbac = True

# Number of seconds to wait before refreshing a batch of workers.
worker_refresh_interval = 180

[celery]
# The Celery broker URL. Celery supports RabbitMQ, Redis and experimentally
# a sqlalchemy database. Refer to the Celery documentation for more
# information.
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#broker-settings
broker_url = redis://airflow-redis.airflow.svc.cluster.local/0

# The Celery result_backend. When a job finishes, it needs to update the
# metadata of the job. Therefore it will post a message on a message bus,
# or insert it into a database (depending of the backend)
# This status is used by the scheduler to update the state of the task
# The use of a database is highly recommended
# http://docs.celeryproject.org/en/latest/userguide/configuration.html#task-result-backend-settings
result_backend = db+mysql://airflow:password@airflow-db.airflow.svc.cluster.local/airflow
